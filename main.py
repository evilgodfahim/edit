import feedparser
import os
import sys
from datetime import datetime, timezone, timedelta
import xml.etree.ElementTree as ET
from xml.dom import minidom
import json
import hashlib

# -----------------------------
# CONFIGURATION
# -----------------------------
FEEDS = [
    "https://politepol.com/fd/pzVBxx3Z2fUI.xml",
    "https://politepol.com/fd/vIzuCnimE1YU.xml",
    "https://politepol.com/fd/QAIWwDi3wOuZ.xml",
    "https://politepol.com/fd/LONi4mJ2tfbd.xml",
    "https://evilgodfahim.github.io/rss-combo-NA/feed.xml",
    "https://politepol.com/fd/2XdgObSDG4FD.xml",
    "https://politepol.com/fd/xaIRlDYPW0kP.xml",
    "https://politepol.com/fd/LwUmZUwUaj7i.xml",
    "https://politepol.com/fd/Uh7pOg6WWCMR.xml",
    "https://politepol.com/fd/GxmRWljxfGEo.xml",
    "https://politepol.com/fd/oT0YgLtnGzze.xml",
    "https://politepol.com/fd/ggpXf4wO5uEz.xml",
    "https://politepol.com/fd/OAVNbKjejtJQ.xml",
    "https://politepol.com/fd/CnOMC37mGwul.xml",
    "https://politepol.com/fd/qVPraFDG1MNh.xml",
    "https://politepol.com/fd/vF2VjeDKWjUw.xml",
    "https://politepol.com/fd/v4jixX1PsBB9.xml",
    "https://politepol.com/fd/NxM7X35BsyKv.xml",
    "https://politepol.com/fd/qJzBCq1mQyIq.xml",
    "https://politepol.com/fd/d3vTXXWIpQfi.xml",
    "https://politepol.com/fd/gXwt22exG6r5.xml",
    "https://politepol.com/fd/wUSywgW7UoCL.xml",
    "https://politepol.com/fd/a18TrHXs0awo.xml",
    "https://politepol.com/fd/nqB5lyvhHzWI.xml",
    "https://evilgodfahim.github.io/ds/articles.xml",
    "https://politepol.com/fd/8R6kYL0taEqD.xml",
    "https://evilgodfahim.github.io/fedit/feed.xml"
]

MASTER_FILE = "feed_master.xml"
DAILY_FILE = "daily_feed.xml"
SEEN_FILE = "seen_ids.json"

MAX_ITEMS = 500
MAX_SEEN_HISTORY = 2000

# -----------------------------
# UTILITIES
# -----------------------------
def clean_html(text):
    if not text:
        return ""
    text = text.replace("≪span class=\"color-red\"≫", "")
    text = text.replace("≪/span≫", "")
    return text

def get_unique_id(entry):
    if hasattr(entry, 'id') and entry.id:
        return entry.id
    if hasattr(entry, 'link') and entry.link:
        return entry.link
    title = getattr(entry, 'title', '')
    published = getattr(entry, 'published', '')
    unique_string = f"{title}{published}"
    return hashlib.md5(unique_string.encode('utf-8')).hexdigest()

def parse_date(entry):
    fields = ["published_parsed", "updated_parsed", "created_parsed"]
    for f in fields:
        t = getattr(entry, f, None)
        if t:
            return datetime(*t[:6], tzinfo=timezone.utc)
    return datetime(1970, 1, 1, tzinfo=timezone.utc)

def extract_source(link):
    try:
        host = link.split("/")[2].lower().replace("www.", "")
        return host.split(".")[0]
    except:
        return "unknown"

# -----------------------------
# XML OPERATIONS
# -----------------------------
def load_existing(path):
    if not os.path.exists(path):
        return []
    try:
        tree = ET.parse(path)
        root = tree.getroot()
        items = []
        for it in root.findall(".//item"):
            try:
                title = it.find("title").text or ""
                link = it.find("link").text or ""
                desc = it.find("description").text or ""
                pub = it.find("pubDate").text or ""
                guid_node = it.find("guid")
                guid = guid_node.text if guid_node is not None else link
                dt = datetime.strptime(pub, "%a, %d %b %Y %H:%M:%S %z")
                items.append({
                    "title": title,
                    "link": link,
                    "description": desc,
                    "pubDate": dt,
                    "id": guid
                })
            except:
                continue
        return items
    except:
        return []

def write_rss(items, path, title="Feed"):
    rss = ET.Element("rss", version="2.0")
    ch = ET.SubElement(rss, "channel")
    ET.SubElement(ch, "title").text = title
    ET.SubElement(ch, "link").text = "https://evilgodfahim.github.io/"
    ET.SubElement(ch, "description").text = f"{title} generated by script"

    for it in items:
        node = ET.SubElement(ch, "item")
        ET.SubElement(node, "title").text = it["title"]
        ET.SubElement(node, "link").text = it["link"]
        ET.SubElement(node, "description").text = it["description"]
        ET.SubElement(node, "pubDate").text = it["pubDate"].strftime("%a, %d %b %Y %H:%M:%S %z")
        guid = ET.SubElement(node, "guid")
        guid.text = it.get("id", it["link"])
        guid.set("isPermaLink", "false")

    xml_str = minidom.parseString(ET.tostring(rss)).toprettyxml(indent="  ")
    with open(path, "w", encoding="utf-8") as f:
        f.write(xml_str)

# -----------------------------
# HASH-BASED UNIQUE TIMESTAMPS
# -----------------------------
def adjust_duplicate_timestamps(items):
    """
    Adjusts timestamps using hash-based offsets for deterministic, stable uniqueness.
    Articles with duplicate timestamps get a consistent offset based on their link hash.
    """
    from collections import defaultdict
    
    # Group items by their original timestamp
    timestamp_groups = defaultdict(list)
    for item in items:
        timestamp_groups[item["pubDate"]].append(item)
    
    # Process each group of duplicates
    for original_dt, group in timestamp_groups.items():
        if len(group) > 1:
            # Sort by link to ensure consistent ordering
            group.sort(key=lambda x: x["link"])
            
            for item in group:
                # Generate hash-based offset (0-59 seconds)
                link_hash = hashlib.md5(item["link"].encode('utf-8')).hexdigest()
                offset_seconds = int(link_hash[:8], 16) % 60
                
                # Apply offset
                item["pubDate"] = original_dt + timedelta(seconds=offset_seconds)
    
    return items

# -----------------------------
# LOGIC: MASTER FEED
# -----------------------------
def update_master():
    existing = load_existing(MASTER_FILE)
    seen_ids = {x["id"] for x in existing}
    new_items = []

    for url in FEEDS:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                try:
                    entry_id = get_unique_id(entry)
                    if entry_id not in seen_ids:
                        link = getattr(entry, "link", "")
                        raw_title = getattr(entry, "title", "No Title")
                        raw_desc = getattr(entry, "summary", "")
                        clean_title = clean_html(raw_title)
                        clean_desc = clean_html(raw_desc)
                        source = extract_source(link)
                        final_title = f"{clean_title}. [ {source} ]"
                        new_items.append({
                            "title": final_title,
                            "link": link,
                            "description": clean_desc,
                            "pubDate": parse_date(entry),
                            "id": entry_id
                        })
                        seen_ids.add(entry_id)
                except:
                    continue
        except:
            continue

    all_items = existing + new_items
    
    # Apply hash-based timestamp adjustments to ensure uniqueness
    all_items = adjust_duplicate_timestamps(all_items)
    
    # Sort by timestamp (newest first) AFTER adjustment
    all_items.sort(key=lambda x: x["pubDate"], reverse=True)
    all_items = all_items[:MAX_ITEMS]

    if not all_items:
        all_items = [{
            "title": "No articles yet",
            "link": "https://evilgodfahim.github.io/",
            "description": "Master feed will populate after first successful fetch.",
            "pubDate": datetime.now(timezone.utc),
            "id": "init_1"
        }]

    write_rss(all_items, MASTER_FILE, "Master Feed (Updated every 30 mins)")

# -----------------------------
# LOGIC: DAILY FEED
# -----------------------------
def update_daily():
    if os.path.exists(SEEN_FILE):
        with open(SEEN_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            history_ids = set(data.get("seen_ids", []))
    else:
        history_ids = set()

    master = load_existing(MASTER_FILE)
    master.sort(key=lambda x: x["pubDate"], reverse=True)

    daily_items = []
    for it in master:
        if it["id"] not in history_ids:
            it["title"] = clean_html(it["title"])
            it["description"] = clean_html(it["description"])
            daily_items.append(it)
            history_ids.add(it["id"])

    if not daily_items:
        daily_items = [{
            "title": "No new articles right now",
            "link": "https://evilgodfahim.github.io/",
            "description": "Check back later.",
            "pubDate": datetime.now(timezone.utc),
            "id": f"msg_{datetime.now().timestamp()}"
        }]

    write_rss(daily_items, DAILY_FILE, "Daily Feed (New Items Only)")

    updated_history = list(history_ids)[-MAX_SEEN_HISTORY:]
    with open(SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump({"seen_ids": updated_history}, f)

# -----------------------------
# MAIN
# -----------------------------
if __name__ == "__main__":
    args = sys.argv[1:]
    if "--master-only" in args:
        update_master()
    elif "--daily-only" in args:
        update_daily()
    else:
        update_master()
        update_daily()