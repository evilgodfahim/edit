import feedparser
import os
import sys
from datetime import datetime, timezone, timedelta
import xml.etree.ElementTree as ET
from xml.dom import minidom
import json
import re

# -----------------------------
# CONFIGURATION
# -----------------------------
FEEDS = [
    "https://politepol.com/fd/pzVBxx3Z2fUI.xml",
    "https://politepol.com/fd/vIzuCnimE1YU.xml",
    "https://politepol.com/fd/QAIWwDi3wOuZ.xml",
    "https://politepol.com/fd/LONi4mJ2tfbd.xml",
    "https://evilgodfahim.github.io/rss-combo-NA/feed.xml",
    "https://politepol.com/fd/2XdgObSDG4FD.xml",
    "https://politepol.com/fd/xaIRlDYPW0kP.xml",
    "https://politepol.com/fd/LwUmZUwUaj7i.xml",
    "https://politepol.com/fd/Uh7pOg6WWCMR.xml",
    "https://politepol.com/fd/GxmRWljxfGEo.xml",
    "https://politepol.com/fd/oT0YgLtnGzze.xml",
    "https://politepol.com/fd/ggpXf4wO5uEz.xml",
    "https://politepol.com/fd/OAVNbKjejtJQ.xml",
    "https://politepol.com/fd/CnOMC37mGwul.xml",
    "https://politepol.com/fd/qVPraFDG1MNh.xml",
    "https://politepol.com/fd/vF2VjeDKWjUw.xml",
    "https://politepol.com/fd/v4jixX1PsBB9.xml",
    "https://politepol.com/fd/NxM7X35BsyKv.xml",
    "https://politepol.com/fd/qJzBCq1mQyIq.xml",
    "https://politepol.com/fd/d3vTXXWIpQfi.xml",
    "https://politepol.com/fd/gXwt22exG6r5.xml",
    "https://politepol.com/fd/wUSywgW7UoCL.xml",
    "https://politepol.com/fd/a18TrHXs0awo.xml",
    "https://politepol.com/fd/nqB5lyvhHzWI.xml",
    "https://evilgodfahim.github.io/ds/articles.xml"
]

MASTER_FILE = "feed_master.xml"
DAILY_FILE = "daily_feed.xml"
LAST_SEEN_FILE = "last_seen.json"

MAX_ITEMS = 500
BD_OFFSET = 6

# -----------------------------
# REMOVE MARKERS
# -----------------------------
def clean_html(text):
    if not text:
        return ""
    text = text.replace("≪span class=\"color-red\"≫", "")
    text = text.replace("≪/span≫", "")
    return text

# -----------------------------
# UTILITIES
# -----------------------------
def parse_date(entry):
    fields = ["published_parsed", "updated_parsed", "created_parsed"]
    for f in fields:
        t = getattr(entry, f, None)
        if t:
            return datetime(*t[:6], tzinfo=timezone.utc)

    # minimal correction: stable UTC fallback
    return datetime.utcnow().replace(tzinfo=timezone.utc)


def load_existing(path):
    if not os.path.exists(path):
        return []
    tree = ET.parse(path)
    root = tree.getroot()
    items = []
    for it in root.findall(".//item"):
        try:
            title = it.find("title").text or ""
            link = it.find("link").text or ""
            desc = it.find("description").text or ""
            pub = it.find("pubDate").text or ""
            dt = datetime.strptime(pub, "%a, %d %b %Y %H:%M:%S %z")
            items.append({"title": title, "link": link, "description": desc, "pubDate": dt})
        except:
            continue
    return items


def write_rss(items, path, title="Feed"):
    rss = ET.Element("rss", version="2.0")
    ch = ET.SubElement(rss, "channel")
    ET.SubElement(ch, "title").text = title
    ET.SubElement(ch, "link").text = "https://evilgodfahim.github.io/"
    ET.SubElement(ch, "description").text = f"{title} generated by script"

    for it in items:
        node = ET.SubElement(ch, "item")
        ET.SubElement(node, "title").text = it["title"]
        ET.SubElement(node, "link").text = it["link"]
        ET.SubElement(node, "description").text = it["description"]
        ET.SubElement(node, "pubDate").text = it["pubDate"].strftime("%a, %d %b %Y %H:%M:%S %z")

    xml_str = minidom.parseString(ET.tostring(rss)).toprettyxml(indent="  ")
    with open(path, "w", encoding="utf-8") as f:
        f.write(xml_str)

# -----------------------------
# SOURCE
# -----------------------------
def extract_source(link):
    try:
        host = link.split("/")[2].lower().replace("www.", "")
        # minimal correction: preserve primary domain consistently
        return host.split(".")[0]
    except:
        return "unknown"

# -----------------------------
# MASTER FEED UPDATE
# -----------------------------
def update_master():
    print("[Updating feed_master.xml]")
    existing = load_existing(MASTER_FILE)
    seen = {x["link"] for x in existing}
    new = []

    for url in FEEDS:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                link = getattr(entry, "link", "")
                if link and link not in seen:
                    raw_title = getattr(entry, "title", "No Title")
                    raw_desc = getattr(entry, "summary", "")
                    clean_title = clean_html(raw_title)
                    clean_desc = clean_html(raw_desc)

                    source = extract_source(link)
                    final_title = f"{clean_title}. [ {source} ]"

                    new.append({
                        "title": final_title,
                        "link": link,
                        "description": clean_desc,
                        "pubDate": parse_date(entry)
                    })
        except Exception as e:
            print(f"Error parsing {url}: {e}")

    all_items = existing + new
    all_items.sort(key=lambda x: x["pubDate"], reverse=True)
    all_items = all_items[:MAX_ITEMS]

    if not all_items:
        all_items = [{
            "title": "No articles yet",
            "link": "https://evilgodfahim.github.io/",
            "description": "Master feed will populate after first successful fetch.",
            "pubDate": datetime.now(timezone.utc)
        }]

    write_rss(all_items, MASTER_FILE, "Master Feed (Updated every 30 mins)")
    print(f"✓ feed_master.xml updated with {len(all_items)} items")

# -----------------------------
# DAILY FEED UPDATE
# -----------------------------
def update_daily():
    print("[Updating daily_feed.xml]")

    to_zone = timezone(timedelta(hours=BD_OFFSET))

    if os.path.exists(LAST_SEEN_FILE):
        with open(LAST_SEEN_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            last_seen = data.get("last_seen")
            last_dt = datetime.fromisoformat(last_seen) if last_seen else None
    else:
        last_dt = None

    master = load_existing(MASTER_FILE)

    new_items = []
    for it in master:
        bd_time = it["pubDate"].astimezone(to_zone)
        if not last_dt or bd_time > last_dt:
            it["title"] = clean_html(it["title"])
            it["description"] = clean_html(it["description"])
            new_items.append(it)

    if not new_items:
        new_items = [{
            "title": "No new articles today",
            "link": "https://evilgodfahim.github.io/",
            "description": "Daily feed will populate after first articles appear.",
            "pubDate": datetime.now(timezone.utc)
        }]

    write_rss(new_items, DAILY_FILE, "Daily Feed (Updated 9 AM BD)")

    last_dt = max([i["pubDate"] for i in new_items])
    with open(LAST_SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump({"last_seen": last_dt.isoformat()}, f)

    print(f"✓ daily_feed.xml updated with {len(new_items)} items")

# -----------------------------
# MAIN
# -----------------------------
if __name__ == "__main__":
    args = sys.argv[1:]
    if "--master-only" in args:
        update_master()
    elif "--daily-only" in args:
        update_daily()
    else:
        update_master()
        update_daily()